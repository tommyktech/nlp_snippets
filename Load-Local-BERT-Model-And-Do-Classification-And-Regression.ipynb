{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ローカルのBERTモデルファイルをロードしてClassificationやRegressionするNotebook\n",
    "Huggingfaceを使う場合はモデル名を指定すればそのモデルをロードできるのでローカルに保存しておいたBERTモデルを使う機会はあまりないですが、<br>\n",
    "新しくリリースされたモデルの場合はまだHuggingfaceに対応しておらずローカルからロードしなければならないこともあると思うので<br>\n",
    "そのための手順を書き残しておきます。<br>\n",
    "<br>\n",
    "ついでに、それを使ってClassificationやRegressionするための手順も書いておきます。<br>\n",
    "\n",
    "参考文献<br>\n",
    "* https://www.tensorflow.org/official_models/fine_tuning_bert\n",
    "* https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_tf_bert.html#TFBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:36:36.551239Z",
     "start_time": "2021-03-15T11:36:34.645647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU') memory growth: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# OOM対策\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
    "else:\n",
    "    print(\"Not enough GPU hardware devices available\")\n",
    "\n",
    "# フォルダの指定\n",
    "bert_folder = \"./uncased_L-12_H-768_A-12\"\n",
    "\n",
    "# Tokenizerのロード\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_folder)\n",
    "\n",
    "# モデルにClassificationのためのDense Layerを追加する\n",
    "class MyTFBertModelForClassification(TFBertModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        # Key名として num_labels を使うとオリジナルのコード内で被ってしまうのでnum_classesにする\n",
    "        num_classes = kwargs.pop('num_classes')\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.drop  = tf.keras.layers.Dropout(0.1)\n",
    "        if num_classes > 1:\n",
    "            self.dense = tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "        else:\n",
    "            self.dense = tf.keras.layers.Dense(num_classes)\n",
    "            \n",
    "    # ClassificationタスクのためにDenseを追加する\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = super().call(inputs, **kwargs)\n",
    "        # dropout layerはなくても動くが汎用性を持たせるために挟んでおく\n",
    "        pooler_output = self.drop(outputs[\"pooler_output\"], training=kwargs['training'])\n",
    "        pooler_output = self.dense(pooler_output)\n",
    "        return pooler_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:36:39.097950Z",
     "start_time": "2021-03-15T11:36:36.551239Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from C:\\Users\\Win7\\tensorflow_datasets\\amazon_us_reviews\\Wireless_v1_00\\0.1.0\n",
      "INFO:absl:Reusing dataset amazon_us_reviews (C:\\Users\\Win7\\tensorflow_datasets\\amazon_us_reviews\\Wireless_v1_00\\0.1.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split ('train[:10%]', 'train[10%:11%]', 'train[11%:12%]'), from C:\\Users\\Win7\\tensorflow_datasets\\amazon_us_reviews\\Wireless_v1_00\\0.1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# データをロードして必要な形式に整える\n",
    "# 今回はAmazonのレビューデータを使う\n",
    "max_seq_length = 128\n",
    "train_data_size = 1000\n",
    "batch_size = 15\n",
    "\n",
    "# とりあえず適当な量のデータをロードする\n",
    "import tensorflow_datasets as tfds\n",
    "train_data, valid_data, test_data = tfds.load(\n",
    "    name=\"amazon_us_reviews/Wireless_v1_00\", \n",
    "    split=('train[:10%]', 'train[10%:11%]', 'train[11%:12%]'),\n",
    ")\n",
    "\n",
    "# データを整形して必要なFieldと数だけ取り出す\n",
    "def cut_off_data(data, limit):\n",
    "    iter_obj = iter(data)\n",
    "    \n",
    "    labels = []\n",
    "    sentences = []\n",
    "    while len(labels) < limit:\n",
    "        example = next(iter_obj)\n",
    "        labels.append(example[\"data\"][\"star_rating\"].numpy())\n",
    "        sentences.append(example[\"data\"][\"review_body\"].numpy())\n",
    "        \n",
    "        cnt = len(labels)\n",
    "        if cnt % 10000 == 0:\n",
    "            print(cnt, end=\" \")\n",
    "    print(len(labels))\n",
    "    \n",
    "    return {\"labels\": labels, \"sentences\":sentences}\n",
    "\n",
    "train_data_dict = cut_off_data(train_data, train_data_size)\n",
    "valid_data_dict = cut_off_data(valid_data, train_data_size / 10)\n",
    "test_data_dict = cut_off_data(test_data, train_data_size / 10)\n",
    "\n",
    "# Tokenizerで文章をトークン化する\n",
    "def encode_sentence(s, tokenizer):\n",
    "    s = str(s)\n",
    "    tokens = list(tokenizer.tokenize(s))\n",
    "    tokens.append('[SEP]')\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "# input_ids, attention_mask, token_type_ids を作るやつ\n",
    "def bert_encode(sentences, tokenizer):\n",
    "    tokenized_sentences = tf.ragged.constant([\n",
    "        encode_sentence(s, tokenizer)[:max_seq_length-1]\n",
    "        for s in sentences])\n",
    "\n",
    "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*tokenized_sentences.shape[0]\n",
    "    input_word_ids = tf.concat([cls, tokenized_sentences], axis=-1)\n",
    "    attention_mask = tf.ones_like(input_word_ids).to_tensor()\n",
    "    type_cls = tf.zeros_like(cls)\n",
    "    type_s1 = tf.zeros_like(tokenized_sentences)\n",
    "    token_type_ids = tf.concat(\n",
    "        [type_cls, type_s1], axis=-1).to_tensor()\n",
    "\n",
    "    inputs = {\n",
    "        'input_ids': input_word_ids.to_tensor(),\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'attention_mask': attention_mask}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# Inputのテキストをエンコードする。あとlabel作る。\n",
    "train_dataset = bert_encode(train_data_dict[\"sentences\"], tokenizer)\n",
    "valid_dataset = bert_encode(valid_data_dict[\"sentences\"], tokenizer)\n",
    "test_dataset  = bert_encode(test_data_dict[\"sentences\"],  tokenizer)\n",
    "train_labels  = np.array(train_data_dict[\"labels\"], dtype=np.int32) - 1\n",
    "valid_labels  = np.array(valid_data_dict[\"labels\"], dtype=np.int32) - 1\n",
    "test_labels   = np.array(test_data_dict[\"labels\"],  dtype=np.int32) - 1\n",
    "\n",
    "train_dataset_batched = tf.data.Dataset.from_tensor_slices((train_dataset, train_labels)).shuffle(100).batch(batch_size).prefetch(1000)# .repeat(2)\n",
    "valid_dataset_batched = tf.data.Dataset.from_tensor_slices((valid_dataset, valid_labels)).shuffle(100).batch(batch_size).prefetch(1000)# .repeat(2)\n",
    "test_dataset_batched  = tf.data.Dataset.from_tensor_slices((test_dataset,  test_labels)).shuffle(100).batch(batch_size).prefetch(1000)# .repeat(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:20:50.591440Z",
     "start_time": "2021-03-15T11:18:16.560035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model MyTFBertModelForClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing MyTFBertModelForClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyTFBertModelForClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model MyTFBertModelForClassification were not initialized from the PyTorch model and are newly initialized: ['dense.weight', 'dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_tf_bert_model_for_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  3845      \n",
      "=================================================================\n",
      "Total params: 109,486,085\n",
      "Trainable params: 109,486,085\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001CC901A6518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001CC901A6518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001CC901A6518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 1.3272 - accuracy: 0.4756WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 52s 637ms/step - loss: 1.3254 - accuracy: 0.4767 - val_loss: 0.9575 - val_accuracy: 0.6800\n",
      "Epoch 2/3\n",
      "67/67 [==============================] - 42s 621ms/step - loss: 0.8797 - accuracy: 0.6816 - val_loss: 0.8422 - val_accuracy: 0.7200\n",
      "Epoch 3/3\n",
      "67/67 [==============================] - 42s 621ms/step - loss: 0.5748 - accuracy: 0.7588 - val_loss: 1.0661 - val_accuracy: 0.6300\n"
     ]
    }
   ],
   "source": [
    "# モデルをロードしてTrainingする\n",
    "import datetime \n",
    "# モデルのロード\n",
    "model = MyTFBertModelForClassification.from_pretrained(bert_folder, from_pt=True, num_classes=5)\n",
    "model.summary()\n",
    "\n",
    "# OptimizerやLossなどの設定\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# tensorboard用の設定\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# training開始\n",
    "hist = model.fit(train_dataset_batched, validation_data=valid_dataset_batched, epochs=3, \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T09:13:03.122476Z",
     "start_time": "2021-03-15T09:13:03.106828Z"
    }
   },
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:29:53.891989Z",
     "start_time": "2021-03-15T11:27:19.860090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model MyTFBertModelForClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing MyTFBertModelForClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MyTFBertModelForClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model MyTFBertModelForClassification were not initialized from the PyTorch model and are newly initialized: ['dense_1.weight', 'dense_1.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_tf_bert_model_for_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  769       \n",
      "=================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001D21B346518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001D21B346518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000001D21B346518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 3.3330 - my_accuracy_fn: 0.2304WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 52s 639ms/step - loss: 3.3201 - my_accuracy_fn: 0.2303 - val_loss: 2.1293 - val_my_accuracy_fn: 0.1571\n",
      "Epoch 2/3\n",
      "67/67 [==============================] - 42s 625ms/step - loss: 1.4765 - my_accuracy_fn: 0.3584 - val_loss: 0.9930 - val_my_accuracy_fn: 0.2571\n",
      "Epoch 3/3\n",
      "67/67 [==============================] - 42s 624ms/step - loss: 0.8436 - my_accuracy_fn: 0.4947 - val_loss: 0.7071 - val_my_accuracy_fn: 0.5810\n"
     ]
    }
   ],
   "source": [
    "# amazon_us_reviewsは本来Classification用のデータだが、star_rating を数値とみなして無理矢理Regressionしてみる\n",
    "# regressionではnum_classesを1にしてモデルをロードする\n",
    "import datetime \n",
    "model = MyTFBertModelForClassification.from_pretrained(bert_folder, from_pt=True, num_classes=1)\n",
    "model.summary()\n",
    "\n",
    "# OptimizerやLossなどの設定\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "# accuracy を計算するための関数を作成する\n",
    "def my_accuracy_fn(y_true, y_pred):\n",
    "    # y_true は 0 から 4までの整数なので、浮動小数点数であるy_predをそれに合わせる\n",
    "    y_pred = tf.where(y_pred < 0.0, 0.0, y_pred)\n",
    "    y_pred = tf.where(y_pred > 4.0, 4.0, y_pred)\n",
    "    y_pred = tf.round(y_pred)\n",
    "    val = tf.cast(tf.math.equal(y_true, y_pred), tf.float32)\n",
    "    return tf.math.reduce_mean(val)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=my_accuracy_fn)\n",
    "\n",
    "# tensorboard用の設定\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# training開始\n",
    "hist = model.fit(train_dataset_batched, validation_data=valid_dataset_batched, epochs=3, \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-11T06:06:52.003910Z",
     "start_time": "2021-03-11T06:06:51.973244Z"
    }
   },
   "source": [
    "### なおHuggingface のTFBertForSequenceClassificationを使えばもっと簡単にClassification、Regressionできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:35:47.722618Z",
     "start_time": "2021-03-15T11:33:31.175098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000258F6D56518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000258F6D56518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x00000258F6D56518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 1.3074 - accuracy: 0.5150WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 52s 634ms/step - loss: 1.3060 - accuracy: 0.5156 - val_loss: 1.0197 - val_accuracy: 0.6300\n",
      "Epoch 2/3\n",
      "67/67 [==============================] - 42s 621ms/step - loss: 0.9430 - accuracy: 0.6560 - val_loss: 0.9140 - val_accuracy: 0.6700\n",
      "Epoch 3/3\n",
      "67/67 [==============================] - 42s 623ms/step - loss: 0.6425 - accuracy: 0.7666 - val_loss: 0.9506 - val_accuracy: 0.6700\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "# classification \n",
    "cls_model = TFBertForSequenceClassification.from_pretrained(bert_folder, from_pt=True, num_labels=5)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "hist = cls_model.fit(train_dataset_batched, validation_data=valid_dataset_batched, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-15T11:38:56.240284Z",
     "start_time": "2021-03-15T11:36:39.097950Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002393F316518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002393F316518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x000002393F316518>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - ETA: 0s - loss: 3.4680 - my_accuracy_fn: 0.1809WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 52s 637ms/step - loss: 3.4463 - my_accuracy_fn: 0.1827 - val_loss: 0.9061 - val_my_accuracy_fn: 0.4381\n",
      "Epoch 2/3\n",
      "67/67 [==============================] - 42s 624ms/step - loss: 0.8820 - my_accuracy_fn: 0.4842 - val_loss: 1.0656 - val_my_accuracy_fn: 0.5381\n",
      "Epoch 3/3\n",
      "67/67 [==============================] - 42s 625ms/step - loss: 0.4768 - my_accuracy_fn: 0.5556 - val_loss: 0.7937 - val_my_accuracy_fn: 0.6095\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# regression では 先ほどと同じようにnum_labels を 1にする\n",
    "reg_model = TFBertForSequenceClassification.from_pretrained(bert_folder, from_pt=True, num_labels=1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "def my_accuracy_fn(y_true, y_pred):\n",
    "    y_pred = tf.where(y_pred < 0.0, 0.0, y_pred)\n",
    "    y_pred = tf.where(y_pred > 4.0, 4.0, y_pred)\n",
    "    y_pred = tf.round(y_pred)\n",
    "    val = tf.cast(tf.math.equal(y_true, y_pred), tf.float32)\n",
    "    return tf.math.reduce_mean(val)\n",
    "reg_model.compile(optimizer=optimizer, loss=loss, metrics=[my_accuracy_fn])\n",
    "\n",
    "hist = reg_model.fit(train_dataset_batched, validation_data=valid_dataset_batched, epochs=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
